# DPO数据集生成工具 - 配置使用说明

## 概述

该工具现在支持在前端动态配置大模型API参数，无需修改后端代码即可连接不同的大模型服务。

## 新增功能

### 1. 大模型配置界面

在前端页面顶部添加了配置区域，包含以下字段：

- **API地址**: 大模型服务的API端点（如: `http://localhost:8888/v1`）
- **API密钥**: 用于认证的密钥（如: `pml_llm`）
- **模型名称**: 要使用的模型名称（如: `Qwen3:BSS`）
- **温度A (chosen)**: 生成"选择"回复的温度参数（0-2，默认0.2）
- **温度B (rejected)**: 生成"拒绝"回复的温度参数（0-2，默认1.0）

### 2. 配置管理功能

- **测试连接**: 验证配置的API是否可用
- **保存配置**: 将配置保存到浏览器本地存储
- **重置配置**: 恢复为默认配置值

### 3. 自动配置验证

- 在提交问题前自动验证配置参数
- 实时显示配置状态和错误信息
- 配置保存在浏览器localStorage中，刷新页面后自动恢复

## 使用流程

### 1. 配置大模型连接

1. 在页面顶部的"大模型配置"区域输入您的API参数
2. 点击"测试连接"验证配置是否正确
3. 测试成功后点击"保存配置"

### 2. 生成DPO数据

1. 确保配置已保存且连接测试成功
2. 在问题输入框中输入您的问题
3. 系统会使用配置的两个不同温度值生成两个候选回复
4. 选择更优的回复或编辑后提交，生成DPO数据对

### 3. 管理数据集

- 点击"编辑数据集"按钮可以查看和编辑已生成的DPO数据
- 支持加载自定义JSON文件路径
- 可以删除、修改单条记录

## 支持的大模型API

该工具支持任何兼容OpenAI ChatCompletions API格式的大模型服务，包括：

- OpenAI GPT系列
- Anthropic Claude (通过代理)
- 本地部署的开源模型 (如Qwen、LLaMA等)
- vLLM、Ollama等推理服务

## API格式要求

确保您的大模型服务支持以下API端点：

```
POST {api_url}/chat/completions
```

请求格式：
```json
{
  "model": "model_name",
  "messages": [{"role": "user", "content": "问题"}],
  "temperature": 0.7,
  "stream": true
}
```

## 配置示例

### 本地vLLM服务
```
API地址: http://localhost:8000/v1
API密钥: pml_llm
模型名称: Qwen/Qwen2-7B-Instruct
```

### Ollama服务
```
API地址: http://localhost:11434/v1
API密钥: ollama
模型名称: qwen2:7b
```

### OpenAI API
```
API地址: https://api.openai.com/v1
API密钥: sk-your-api-key
模型名称: gpt-3.5-turbo
```

## 故障排除

### 连接测试失败

1. **检查API地址**: 确保地址格式正确且服务正在运行
2. **验证API密钥**: 确保密钥有效且具有必要权限
3. **确认模型名称**: 检查模型名称在服务中是否可用
4. **网络连接**: 确保前端可以访问API服务

### 生成回复失败

1. **配置验证**: 确保所有配置字段都已正确填写
2. **温度值**: 确保温度值在0-2之间
3. **API配额**: 检查API是否有使用限制或配额耗尽

### 数据保存问题

1. **浏览器存储**: 确保浏览器允许localStorage
2. **文件权限**: 确保后端有权限写入数据集文件
3. **磁盘空间**: 确保有足够的磁盘空间保存数据

## 技术细节

- 前端使用localStorage保存配置
- 配置在每次API调用时动态传递给后端
- 后端支持向下兼容，未传递配置时使用默认值
- 流式响应支持实时显示生成进度
